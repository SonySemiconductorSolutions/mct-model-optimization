{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f8194007-6ea7-4e00-8931-a37ca2d0dd20",
      "metadata": {
        "id": "f8194007-6ea7-4e00-8931-a37ca2d0dd20"
      },
      "source": [
        "# Activation Threshold Demonstration For Post-Training Quantization\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9be59ea8-e208-4b64-aede-1dd6270b3540",
      "metadata": {
        "id": "9be59ea8-e208-4b64-aede-1dd6270b3540"
      },
      "source": [
        "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/keras/ptq/example_keras_mobilenet_mixed_precision.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "930e6d6d-4980-4d66-beed-9ff5a494acf9",
      "metadata": {
        "id": "930e6d6d-4980-4d66-beed-9ff5a494acf9"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "699be4fd-d382-4eec-9d3f-e2e85cfb1762",
      "metadata": {
        "id": "699be4fd-d382-4eec-9d3f-e2e85cfb1762"
      },
      "source": [
        "This tutorial demonstrates the process used to find the activation threshold, a step that MCT uses during post-training quantisation.\n",
        "\n",
        "In this example, for a single activation layer. We will run 2 methods of mct quantisation, feed a representative dataset through the model, plot the activation distribution of two layers with their respective mct calculated thresholds and finally compare the quantised model accuracy of the two methods.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85199e25-c587-41b1-aaf5-e1d23ce97ca1",
      "metadata": {
        "id": "85199e25-c587-41b1-aaf5-e1d23ce97ca1"
      },
      "source": [
        "## Activation threshold explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a89a17f4-30c9-4caf-a888-424f7a82fbc8",
      "metadata": {
        "id": "a89a17f4-30c9-4caf-a888-424f7a82fbc8"
      },
      "source": [
        "\n",
        "Quantisation thresholds are used to map a distribution of 32bit float values to their 8bit quantised counterparts. Doing this with the least loss of data while maintaining the most representative range is important for final model accuracy.\n",
        "\n",
        "\n",
        "MCT's Post-training quantisation uses a represenative dataset to evaluate a list of typical output activation values. The challenge comes with how best to match these values to their quantised counterparts. This process comprises of two main steps, zscore threshold, quantisation threshold. Initially anomolus values must be removed these values may result in the final threshold being greater than it needs to be (reducing granularity of mapping and there for also reducing accuracy of model).\n",
        "Here MCT uses z-score thresholding on the activation values establishing a threshold value to remove anomolus values. The process by which this is used will be covered in another tutorial.\n",
        "\n",
        "Quantisation Threshold MCT has a number of error metrics for finding the best quantisation threshold. However, Mean squared error is typically the best performing and used by default.\n",
        "\n",
        "The error is calculated based on the difference between the float and quantised distribution. The threshold is sellected based on the minimum error. For the case of MSE;\n",
        "\n",
        "$$\n",
        "ERR(t) = \\frac{1}{n_s} \\sum_{X \\in Fl(D)} (Q(X, t, n_b) - X)^2\n",
        "$$\n",
        "\n",
        "$ERR(t)$ : The quantization error function dependent on threshold t.\n",
        "ns: The size of the representative dataset, indicating normalization over the dataset's size.\n",
        "\n",
        "$\\sum$: Summation over all elements X in the flattened dataset $Fl(D)$.\n",
        "\n",
        "$F_l(D)$: The collection of activation tensors in the l-th layer, representing the dataset D flattened for processing.\n",
        "\n",
        "$Q(X, t, n_b)$: The quantized approximation of X, given a threshold t and bit width nb.\n",
        "\n",
        "$X$: The original activation tensor before quantization.\n",
        "\n",
        "$t$: The quantization threshold, a critical parameter for controlling the quantization process.\n",
        "\n",
        "$n_b$: The number of bits used in the quantization process, affecting the model's precision and size.\n",
        "\n",
        "To increase efficiency in calculating the threshold, the search space for best threshold is restricted to **Power of Two** values only. This both restricts number of potential values to a reasonable number and increases hardware efficency.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c0e9543-d356-412f-acf1-c2ecad553e06",
      "metadata": {
        "id": "9c0e9543-d356-412f-acf1-c2ecad553e06"
      },
      "source": [
        "Error methods supported by MCT;\n",
        "\n",
        "NOCLIPPING - Use min/max values as thresholds.\n",
        "\n",
        "MSE - Use min square error for minimizing quantizationnoises.\n",
        "\n",
        "MAE - Use min absolute error for minimizing quantization nose.\n",
        "\n",
        "KL - Use KL-divergen ce tosgnals disb as tas o be similar as posible.\n",
        "\n",
        "Lp - Use Lpsingimizing quantization noise."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04228b7c-00f1-4ded-bead-722e2a4e89a0",
      "metadata": {
        "id": "04228b7c-00f1-4ded-bead-722e2a4e89a0",
        "tags": []
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2657cf1a-654d-45a6-b877-8bf42fc26d0d",
      "metadata": {
        "id": "2657cf1a-654d-45a6-b877-8bf42fc26d0d"
      },
      "source": [
        "Install and import the relevant packages:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "324685b9-5dcc-4d22-80f4-dec9a93d3324",
      "metadata": {
        "id": "324685b9-5dcc-4d22-80f4-dec9a93d3324",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow\n",
        "!pip install -q mct-nightly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3f0acc8-281c-4bca-b0b9-3d7677105f19",
      "metadata": {
        "id": "b3f0acc8-281c-4bca-b0b9-3d7677105f19"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import model_compression_toolkit as mct\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone MCT to gain access to tutorial scripts"
      ],
      "metadata": {
        "id": "z8F-avk3azgZ"
      },
      "id": "z8F-avk3azgZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3b675cf-e1b5-4249-a581-ffb9b1c16ba1",
      "metadata": {
        "id": "e3b675cf-e1b5-4249-a581-ffb9b1c16ba1"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/sony/model_optimization.git local_mct\n",
        "!pip install -r ./local_mct/requirements.txt\n",
        "import sys\n",
        "sys.path.insert(0,\"./local_mct\")\n",
        "import tutorials.resources.utils.keras_tutorial_tools as tutorial_tools"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c7fed0d-cfc8-41ee-adf1-22a98110397b",
      "metadata": {
        "id": "0c7fed0d-cfc8-41ee-adf1-22a98110397b"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aecde59e4c37b1da",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "aecde59e4c37b1da"
      },
      "source": [
        "Load imagenet classification dataset and seperate a small representative subsection of this dataset to use for quantisation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.isdir('imagenet'):\n",
        "    !mkdir imagenet\n",
        "    !wget https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz\n",
        "    !mv ILSVRC2012_devkit_t12.tar.gz imagenet/\n",
        "    !wget https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar\n",
        "    !mv ILSVRC2012_img_val.tar imagenet/"
      ],
      "metadata": {
        "id": "_ztv72uM6-UT"
      },
      "id": "_ztv72uM6-UT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-lnS4j-s1PeR"
      },
      "id": "-lnS4j-s1PeR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "if not os.path.isdir('imagenet/val'):\n",
        "    ds = torchvision.datasets.ImageNet(root='./imagenet', split='val')"
      ],
      "metadata": {
        "id": "YVAoUjK47Zcp"
      },
      "id": "YVAoUjK47Zcp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "fcbb3eecae5346a9",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "fcbb3eecae5346a9"
      },
      "source": [
        "Here we create the representative dataset. For detail on this step see imagenet tutorial. If you are running locally a higher fraction of the dataset can be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eda9ad33-f88c-4178-8f19-bac6b2b2e97b",
      "metadata": {
        "id": "eda9ad33-f88c-4178-8f19-bac6b2b2e97b"
      },
      "outputs": [],
      "source": [
        "REPRESENTATIVE_DATASET_FOLDER = './imagenet/val'\n",
        "BATCH_SIZE = 20\n",
        "fraction =0.001\n",
        "representative_dataset_gen = tutorial_tools.get_representative_dataset(fraction, REPRESENTATIVE_DATASET_FOLDER, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a1e9ba6-2954-4506-ad5c-0da273701ba5",
      "metadata": {
        "id": "4a1e9ba6-2954-4506-ad5c-0da273701ba5"
      },
      "source": [
        "## MCT quantisation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55edbb99-ab2f-4dde-aa74-4ddee61b2615",
      "metadata": {
        "id": "55edbb99-ab2f-4dde-aa74-4ddee61b2615"
      },
      "source": [
        "This step we load the model and quantise with two methods of threshold error calculation: no clipping and MSE.\n",
        "\n",
        "No clipping chooses the lowest Power of two threshold that does not loose any data to its threshold.\n",
        "\n",
        "MSE chooses a Power of two threshold that results in the least difference between the float distribution and the quantised distribution.\n",
        "\n",
        "This means no clipping will often result in a larger threshold, which we will see later in this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we load mobilenetv2 from the keras library"
      ],
      "metadata": {
        "id": "VMrcPUN6jPlB"
      },
      "id": "VMrcPUN6jPlB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c431848f-a5f4-4737-a5c8-f046a8bca840",
      "metadata": {
        "id": "c431848f-a5f4-4737-a5c8-f046a8bca840"
      },
      "outputs": [],
      "source": [
        "from keras.applications.mobilenet_v2 import MobileNetV2\n",
        "float_model = MobileNetV2()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantisation perameters are defined."
      ],
      "metadata": {
        "id": "Pd8blHyKjWay"
      },
      "id": "Pd8blHyKjWay"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca971297-e00b-44b5-b9e1-e57ba5843e38",
      "metadata": {
        "id": "ca971297-e00b-44b5-b9e1-e57ba5843e38"
      },
      "outputs": [],
      "source": [
        "from model_compression_toolkit import QuantizationErrorMethod\n",
        "\n",
        "# Specify the IMX500-v1 target platform capability (TPC)\n",
        "tpc = mct.get_target_platform_capabilities(\"tensorflow\", 'imx500', target_platform_version='v1')\n",
        "\n",
        "# Set the following quantization configurations:\n",
        "# Choose the desired QuantizationErrorMethod for the quantization parameters search.\n",
        "# Enable weights bias correction induced by quantization.\n",
        "# Enable shift negative corrections for improving 'signed' non-linear functions quantization (such as swish, prelu, etc.)\n",
        "# Set the threshold to filter outliers with z_score of 16.\n",
        "\n",
        "# List of error methods to iterate over\n",
        "q_configs_dict = {}\n",
        "\n",
        "# Common parameters\n",
        "weights_bias_correction = True\n",
        "shift_negative_activation_correction = True\n",
        "z_threshold = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can edit the code below to quantise with other error metrics MCT supports."
      ],
      "metadata": {
        "id": "Vot-MCiWjzCE"
      },
      "id": "Vot-MCiWjzCE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Error methods to iterate over\n",
        "error_methods = [\n",
        "    QuantizationErrorMethod.MSE,\n",
        "    QuantizationErrorMethod.NOCLIPPING\n",
        "]\n",
        "\n",
        "# If you are curious you can add any of the below quantisation methods as well.\n",
        "#QuantizationErrorMethod.MAE\n",
        "#QuantizationErrorMethod.KL\n",
        "#QuantizationErrorMethod.LP\n",
        "\n",
        "# Iterate and build the QuantizationConfig objects\n",
        "for error_method in error_methods:\n",
        "    q_config = mct.QuantizationConfig(\n",
        "        activation_error_method=error_method,\n",
        "        weights_error_method=error_method,\n",
        "        weights_bias_correction=weights_bias_correction,\n",
        "        shift_negative_activation_correction=shift_negative_activation_correction,\n",
        "        z_threshold=z_threshold\n",
        "    )\n",
        "\n",
        "    q_configs_dict[error_method] = q_config"
      ],
      "metadata": {
        "id": "jtiZzXmTjxuI"
      },
      "id": "jtiZzXmTjxuI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we quantise the model, this can take some time."
      ],
      "metadata": {
        "id": "8W3Dcn0jkJOH"
      },
      "id": "8W3Dcn0jkJOH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba0c6e55-d474-4dc3-9a43-44b736635998",
      "metadata": {
        "id": "ba0c6e55-d474-4dc3-9a43-44b736635998"
      },
      "outputs": [],
      "source": [
        "quantized_models_dict = {}\n",
        "\n",
        "for error_method, q_config in q_configs_dict.items():\n",
        "    # Create a CoreConfig object with the current quantization configuration\n",
        "    ptq_config = mct.core.CoreConfig(quantization_config=q_config)\n",
        "\n",
        "    # Perform MCT post-training quantization\n",
        "    quantized_model, quantization_info = mct.ptq.keras_post_training_quantization_experimental(\n",
        "        in_model=float_model,\n",
        "        representative_data_gen=representative_dataset_gen,\n",
        "        core_config=ptq_config,\n",
        "        target_platform_capabilities=tpc\n",
        "    )\n",
        "\n",
        "    # Update the dictionary to include the quantized model\n",
        "    quantized_models_dict[error_method] = {\n",
        "        \"quantization_config\": q_config,\n",
        "        \"quantized_model\": quantized_model,\n",
        "        \"quantization_info\": quantization_info\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Threshold and Distribution Visulisation"
      ],
      "metadata": {
        "id": "A8UHRsh2khM4"
      },
      "id": "A8UHRsh2khM4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To assist with understanding we will now plot the threshold and distributuion from two of Mobilenetv2's layers.\n",
        "\n",
        "MCT quantisation_info stores threshold data per layer. However, to see the distribution of the activations the model needs to be rebuilt upto and including the layer chosen for distribution visulisation.\n",
        "\n",
        "To do this we first need to list the layer names. With keras this can be done easily for the first 10 layes with the following."
      ],
      "metadata": {
        "id": "Y-0QLWFJkpFV"
      },
      "id": "Y-0QLWFJkpFV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a22e6d68-c40f-40bf-ab74-ff453011aeac",
      "metadata": {
        "id": "a22e6d68-c40f-40bf-ab74-ff453011aeac"
      },
      "outputs": [],
      "source": [
        "for index, layer in enumerate(float_model.layers):\n",
        "    if index < 10:\n",
        "        print(layer.name)\n",
        "    else:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c38d28f3-c947-4c7c-aafa-e96cc3864277",
      "metadata": {
        "id": "c38d28f3-c947-4c7c-aafa-e96cc3864277"
      },
      "source": [
        "First activation layer in model is 'Conv1_relu'.\n",
        "\n",
        "For this particular model expanded_conv_project_BN demonstrates the difference between the two error metrics so we will also use this.\n",
        "\n",
        "Use these layer names to create a pair of models that end in these respective layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f9dd3f3-6e22-4be9-9beb-29568ff14c9d",
      "metadata": {
        "id": "1f9dd3f3-6e22-4be9-9beb-29568ff14c9d"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "layer_name1 = 'Conv1_relu'\n",
        "layer_name2 = 'expanded_conv_project_BN'\n",
        "\n",
        "layer_output1 = float_model.get_layer(layer_name1).output\n",
        "activation_model_relu = Model(inputs=float_model.input, outputs=layer_output1)\n",
        "layer_output2 = float_model.get_layer(layer_name2).output\n",
        "activation_model_project = Model(inputs=float_model.input, outputs=layer_output2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccc81508-01e5-421c-9b48-6ed3ce5b7364",
      "metadata": {
        "id": "ccc81508-01e5-421c-9b48-6ed3ce5b7364"
      },
      "source": [
        "Feed the representative dataset through these models and store the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaeb9888-5d67-4979-af50-80781a811b4b",
      "metadata": {
        "id": "eaeb9888-5d67-4979-af50-80781a811b4b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "activation_batches_relu = []\n",
        "activation_batches_project = []\n",
        "for images in representative_dataset_gen():\n",
        "    activations_relu = activation_model_relu.predict(images)\n",
        "    activation_batches_relu.append(activations_relu)\n",
        "    activations_project = activation_model_project.predict(images)\n",
        "    activation_batches_project.append(activations_project)\n",
        "\n",
        "all_activations_relu = np.concatenate(activation_batches_relu, axis=0).flatten()\n",
        "all_activations_project = np.concatenate(activation_batches_project, axis=0).flatten()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thresholds calculated by MCT during quantisation can be accessed using the following. The layer number matches the index of the layers named in the previous steps.\n",
        "\n",
        "As mentioned above we use the first activation relu layer and the batch normalisation layer as they best demonstrate the effect of the two threshold error methods."
      ],
      "metadata": {
        "id": "I5W9yY5DvOFr"
      },
      "id": "I5W9yY5DvOFr"
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_thresholds_relu = {error_method: data[\"quantized_model\"].layers[4].activation_holder_quantizer.get_config()['threshold'][0] for error_method, data in quantized_models_dict.items()}\n",
        "optimal_thresholds_project = {error_method: data[\"quantized_model\"].layers[9].activation_holder_quantizer.get_config()['threshold'][0] for error_method, data in quantized_models_dict.items()}"
      ],
      "metadata": {
        "id": "NGnjrPD_uTd5"
      },
      "id": "NGnjrPD_uTd5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distribution Plots\n",
        "\n",
        "These are the distributions of the two layers firstly, below relu and secondly Project_BN.\n",
        "\n",
        "The second distribution shows distinctly the difference in the result of the two error metrics."
      ],
      "metadata": {
        "id": "XRAr8L5mvuLd"
      },
      "id": "XRAr8L5mvuLd"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "optimal_thresholds_relu = {error_method: data[\"quantized_model\"].layers[4].activation_holder_quantizer.get_config()['threshold'][0] for error_method, data in quantized_models_dict.items()}\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(all_activations_relu, bins=100, alpha=0.5, label='Original')\n",
        "for method, threshold in optimal_thresholds_relu.items():\n",
        "    plt.axvline(threshold, linestyle='--', linewidth=2, label=f'{method}: {threshold:.2f}')\n",
        "\n",
        "plt.title('Activation Distribution with Optimal Quantization Thresholds First Relu Layer')\n",
        "plt.xlabel('Activation Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VPb8tBNGpJjo"
      },
      "id": "VPb8tBNGpJjo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(all_activations_project, bins=100, alpha=0.5, label='Original')\n",
        "for method, threshold in optimal_thresholds_project.items():\n",
        "    plt.axvline(threshold, linestyle='--', linewidth=2, label=f'{method}: {threshold:.2f}')\n",
        "\n",
        "plt.title('Activation Distribution with Optimal Quantization Thresholds Prohject BN layer')\n",
        "plt.xlabel('Activation Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Df7eKzh4oj5X"
      },
      "id": "Df7eKzh4oj5X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4c967d41-439d-405b-815f-be641f1768fe",
      "metadata": {
        "id": "4c967d41-439d-405b-815f-be641f1768fe"
      },
      "source": [
        "## Accuracy\n",
        "\n",
        "Finally we can show the effect of these different thresholds on the models accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "092d9fd0-8005-4551-b853-3b52840639c2",
      "metadata": {
        "id": "092d9fd0-8005-4551-b853-3b52840639c2"
      },
      "outputs": [],
      "source": [
        "TEST_DATASET_FOLDER = './imagenet/val'\n",
        "evaluation_dataset = tutorial_tools.get_validation_dataset_fraction(0.005, TEST_DATASET_FOLDER, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ebf7d04-7816-465c-9157-6068c0a4a08a",
      "metadata": {
        "id": "8ebf7d04-7816-465c-9157-6068c0a4a08a"
      },
      "outputs": [],
      "source": [
        "float_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])\n",
        "results = float_model.evaluate(evaluation_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07a22d28-56ff-46de-8ed0-1163c3b7a613",
      "metadata": {
        "id": "07a22d28-56ff-46de-8ed0-1163c3b7a613"
      },
      "outputs": [],
      "source": [
        "evaluation_results = {}\n",
        "\n",
        "for error_method, data in quantized_models_dict.items():\n",
        "    quantized_model = data[\"quantized_model\"]\n",
        "\n",
        "    quantized_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])\n",
        "\n",
        "    results = quantized_model.evaluate(evaluation_dataset, verbose=0)  # Set verbose=0 to suppress the log messages\n",
        "\n",
        "    evaluation_results[error_method] = results\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"Results for {error_method}: Loss = {results[0]}, Accuracy = {results[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These results mirror the case for many models hence why MSE has been chosen by default by the MCT team.\n",
        "\n",
        "Each of MCT's error methods have a different effect on different models so it is always worth including this metric into hyper perameter tuning when trying to improve quantised model accuracy."
      ],
      "metadata": {
        "id": "GpEZ2E1qzWl3"
      },
      "id": "GpEZ2E1qzWl3"
    },
    {
      "cell_type": "markdown",
      "id": "14877777",
      "metadata": {
        "id": "14877777"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb7e1572",
      "metadata": {
        "id": "bb7e1572"
      },
      "source": [
        "In this tutorial, we demonstrated the methods used to find a layers quantisation threshold for activation. The process is similar for weight quantisation but a representative dataset is not required. Use this code to assist with choosing error methods for your own model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c0c9b61-8056-4d06-8a2b-6e5fc56325f6",
      "metadata": {
        "id": "8c0c9b61-8056-4d06-8a2b-6e5fc56325f6"
      },
      "source": [
        "## Appendix\n",
        "\n",
        "Some code to assist with gaining information from each layer in the MCT quanisation output."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import inspect\n",
        "\n",
        "\n",
        "quantized_model = data[\"quantized_model\"]\n",
        "quantizer_object = quantized_model.layers[1]\n",
        "\n",
        "quantized_model = data[\"quantized_model\"]\n",
        "\n",
        "\n",
        "relu_layer_indices = []\n",
        "\n",
        "\n",
        "for i, layer in enumerate(quantized_model.layers):\n",
        "    # Convert the layer's configuration to a string\n",
        "    layer_config_str = str(layer.get_config())\n",
        "\n",
        "    layer_class_str = str(layer.__class__.__name__)\n",
        "\n",
        "    # Check if \"relu\" is mentioned in the layer's configuration or class name\n",
        "    if 'relu' in layer_config_str.lower() or 'relu' in layer_class_str.lower():\n",
        "        relu_layer_indices.append(i)\n",
        "\n",
        "print(\"Layer indices potentially using ReLU:\", relu_layer_indices)\n",
        "print(\"Number of relu layers \" + str(len(relu_layer_indices)))\n"
      ],
      "metadata": {
        "id": "qml4LLmWZLP4"
      },
      "id": "qml4LLmWZLP4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43f34133-8ed4-429a-a225-6fb6a6f5b207",
      "metadata": {
        "id": "43f34133-8ed4-429a-a225-6fb6a6f5b207"
      },
      "outputs": [],
      "source": [
        "for error_method, data in quantized_models_dict.items():\n",
        "    quantized_model = data[\"quantized_model\"]\n",
        "    print(quantized_model.layers[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01c1645e-205c-4d9a-8af3-e497b3addec1",
      "metadata": {
        "id": "01c1645e-205c-4d9a-8af3-e497b3addec1"
      },
      "source": [
        "\n",
        "\n",
        "Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}