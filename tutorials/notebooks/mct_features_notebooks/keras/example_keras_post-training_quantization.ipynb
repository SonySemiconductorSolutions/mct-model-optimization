{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Post-Training Quantization in Keras using the Model Compression Toolkit (MCT)\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/mct_features_notebooks/keras/example_keras_post_training_quantization.ipynb)\n",
    "\n",
    "## Overview\n",
    "This quick-start guide explains how to use the **Model Compression Toolkit (MCT)** to quantize a Keras model. We will load a pre-trained model and  quantize it using the MCT with **Post-Training Quatntization (PTQ)**. Finally, we will evaluate the quantized model and export it to a Keras or TFLite files.\n",
    "\n",
    "## Summary\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "1. Loading and preprocessing the Imagenette dataset using the Tensorflow Datasets package.\n",
    "2. Constructing an unlabeled representative dataset.\n",
    "3. Hardware-Friendly Post-Training Quantization using MCT.\n",
    "4. Accuracy evaluation of the floating-point and the quantized models.\n",
    "5. Exporting the model to Keras and TFLite files.\n",
    "\n",
    "## Setup\n",
    "Install the relevant packages:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37caa075419872cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TF_VER = '2.14'\n",
    "!pip install -q tensorflow[and-cuda]~={TF_VER} tensorflow-datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2227c2812088b426"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import importlib\n",
    "if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "    !pip install model_compression_toolkit"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1849396447aa75e8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96817134aaa61465"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load a pre-trained MobileNetV2 model from Keras, in 32-bits floating-point precision format."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0d72559f34c030a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "\n",
    "float_model = MobileNetV2()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14f37bd9e1421650"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset preparation\n",
    "### Download ImageNette validation set\n",
    "For this demonstration, we will use the Imagenette dataset, a subset of 10 easily classified classes from the larger ImageNet dataset.\n",
    "\n",
    "**Note** that for demonstration purposes we use the validation set for the model quantization routines. Typically, a subset of the training dataset is used, but loading it is a heavy procedure that is unnecessary for this example.\n",
    "\n",
    "Load the Imagenette validation dataset using tensorflow-datasets:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8fac30930c364bb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "imagenette_class_indices = [0, 217, 482, 491, 497, 566, 569, 571, 574, 701]  # ImageNet indices for Imagenette classes\n",
    "\n",
    "# Load the Imagenette validation split\n",
    "imagenet_val_ds, info = tfds.load('imagenette', split='validation', data_dir='./imagenette', with_info=True, as_supervised=True)\n",
    "\n",
    "# Preprocess the dataset\n",
    "img_size = 224  # Model's expected input size\n",
    "batch_size = 50\n",
    "\n",
    "def preprocess_image(image, label, img_size):\n",
    "    image = tf.image.resize(image, (img_size, img_size))\n",
    "    image = preprocess_input(image)  # Preprocess using MobileNetV2's preprocessing\n",
    "    return image, label\n",
    "\n",
    "val_ds = imagenet_val_ds.map(lambda img, lbl: preprocess_image(img, lbl, img_size), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70e23b77b41fc6e1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Representative Dataset\n",
    "For quantization with MCT, we need to define a representative dataset required by the PTQ algorithm. This dataset is a generator that returns a list of images:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab88ee0beaff2186"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "n_iter = 5\n",
    "\n",
    "numpy_dataset = tfds.as_numpy(val_ds)\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    dataloader_iter = iter(numpy_dataset)\n",
    "    for _ in range(n_iter):\n",
    "        yield [next(dataloader_iter)[0]]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c164088f1882bad8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Target Platform Capabilities\n",
    "MCT optimizes the model for dedicated hardware. This is done using TPC (for more details, please visit our [documentation](https://sony.github.io/model_optimization/docs/api/api_docs/modules/target_platform.html)). Here, we use the default Tensorflow TPC:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7cf37cd66fca511"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import model_compression_toolkit as mct\n",
    "\n",
    "# Get a TargetPlatformCapabilities object that models the hardware for the quantized model inference. Here, for example, we use the default platform that is attached to a Keras layers representation.\n",
    "target_platform_cap = mct.get_target_platform_capabilities('tensorflow', 'default')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "259e2cf078cd3dfe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hardware-Friendly Post-Training Quantization using MCT\n",
    "Now for the exciting part! Letâ€™s run hardware-friendly PTQ on the model. \n",
    "**Hardware-friendly** means symmetric quantization with power-of-2 thresholds."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c19234f699c75374"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "quantized_model, quantization_info = mct.ptq.keras_post_training_quantization(\n",
    "        in_model=float_model,\n",
    "        representative_data_gen=representative_dataset_gen,\n",
    "        target_platform_capabilities=target_platform_cap\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a791d320d064f950"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our model is now quantized. MCT has created a simulated quantized model within the original Keras framework by inserting [quantization representation modules](https://github.com/sony/mct_quantizers). These modules, such as `KerasQuantizationWrapper` and `KerasActivationQuantizationHolder`, wrap Keras layers to simulate the quantization of weights and activations, respectively. While the size of the saved model remains unchanged, all the quantization parameters are stored within these modules and are ready for deployment on the target hardware. In this example, we used the default MCT settings, which compressed the model from 32 bits to 8 bits, resulting in a compression ratio of 4x."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "877eef17e44c57c3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Evaluation\n",
    "In order to evaluate our models, we first need to define a function for evaluation of a Keras model trained on ImageNet using the Imagenette dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bac59bdc7eb51d15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_imagenette_model(model, dataset, class_indices):\n",
    "\n",
    "    # Initialize variables to store predictions and ground truth labels\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for images, true_labels in tqdm(dataset, total=len(dataset), desc=\"Evaluating\"):\n",
    "        # Make predictions with the pre-trained model\n",
    "        preds = model.predict(images, verbose=0)\n",
    "\n",
    "        # Extract predictions only for the Imagenette classes\n",
    "        imagenette_preds = preds[:, class_indices]\n",
    "\n",
    "        # Map predictions to the highest scoring Imagenette class\n",
    "        predicted_classes = np.argmax(imagenette_preds, axis=1)\n",
    "\n",
    "        # Compare predictions with true labels\n",
    "        correct_predictions += np.sum(predicted_classes == true_labels.numpy())\n",
    "        total_samples += true_labels.shape[0]\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return accuracy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8af62e22913de3e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's start with the floating-point model evaluation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9455ac334c8c23da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "float_accuracy = evaluate_imagenette_model(float_model, val_ds, imagenette_class_indices)\n",
    "print(f\"Float model's accuracy on Imagenette: {(float_accuracy * 100):.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b25fabd1923b4a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, let's evaluate the quantized model:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3085f431ccc6fdee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "quant_accuracy = evaluate_imagenette_model(quantized_model, val_ds, imagenette_class_indices)\n",
    "print(f\"Quantized model's accuracy on Imagenette: {(quant_accuracy * 100):.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c427527160845924"
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can see that we got a very small degradation with a compression rate of x4 !\n",
    "Now, we can export the quantized model to Keras and TFLite:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "959526e5ae914e6b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mct.exporter.keras_export_model(\n",
    "    model=quantized_model,\n",
    "    save_model_path='qmodel.tflite',\n",
    "    serialization_format=mct.exporter.KerasExportSerializationFormat.TFLITE,\n",
    "    quantization_format=mct.exporter.QuantizationFormat.FAKELY_QUANT)\n",
    "\n",
    "mct.exporter.keras_export_model(model=quantized_model, save_model_path='qmodel.keras')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d431b13e8ac5e4d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we demonstrated how to quantize a classification model in a hardware-friendly manner using MCT. We observed that a 4x compression ratio was achieved with minimal performance degradation.\n",
    "\n",
    "The key advantage of hardware-friendly quantization is that the model can run more efficiently in terms of runtime, power consumption, and memory usage on designated hardware.\n",
    "\n",
    "While this was a simple model and task, MCT can deliver competitive results across a wide range of tasks and network architectures. For more details, [check out the paper:](https://arxiv.org/abs/2109.09113).\n",
    "\n",
    "## Copyrights\n",
    "\n",
    "Copyright 2024 Sony Semiconductor Israel, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78c7a00d0acb623d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "58ce5ed1d89905ba"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
