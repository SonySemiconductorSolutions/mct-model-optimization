{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Post Training Quantization an EfficientDet Object Detection Model\n",
    "\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/example_keras_effdet_lite0.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we'll demonstrate the post-training quantization using MCT for a pre-trained object detection model in Keras. In addition, we'll integrate a post-processing custom layer from [sony-custom-layers](https://github.com/sony/custom_layers) into the model. This integration aligns with the imx500 target platform capabilities.\n",
    "\n",
    "In this example we will use an existing pre-trained EfficientDet model taken from [efficientdet-pytorch](https://github.com/rwightman/efficientdet-pytorch). We will convert the model to a Keras functional model that includes the custom [PostProcess Layer](https://github.com/sony/custom_layers/blob/main/sony_custom_layers/keras/object_detection/ssd_post_process.py). Further, we will quantize the model using MCT post training quantization and evaluate the performance of the floating point model and the quantized model on the COCO dataset.\n",
    "\n",
    "We'll use the [timm](https://github.com/huggingface/pytorch-image-models)'s data loader and evaluation capabilities used for the original pytorch pretrained model. The conversion to the Keras model will not be covered. You can go over the conversion [here](https://github.com/sony/model_optimization/tree/main/tutorials/resources/efficientdet)\n",
    "\n",
    "Steps:\n",
    "* **Setup environment**: install relevant packages, import them\n",
    "* **Init dataset**: Download the COCO evaluation and prepare the evaluation code\n",
    "* **Keras float model**: Create the Keras model, assign the pretrained weights and evaluate it\n",
    "* **Quantize Keras mode**: Quantize the model and evaluate it\n",
    "\n",
    "**Note**: The following code should be run on a GPU."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9e7b10d2bfe67d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "install and import relevant packages"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0e81b09e6d30873"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -q tensorflow\n",
    "!pip install -q model-compression-toolkit\n",
    "!pip install -q torch\n",
    "!pip install -q torchvision\n",
    "!pip install -q timm\n",
    "!pip install -q effdet\n",
    "!pip install -q sony-custom-layers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6695a3ec84402e29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from time import time\n",
    "import torch\n",
    "from timm.utils import AverageMeter\n",
    "from effdet.config import get_efficientdet_config\n",
    "from effdet import create_dataset, create_loader, create_evaluator\n",
    "from effdet.data import resolve_input_config\n",
    "import model_compression_toolkit as mct"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "735aee910cf92d42"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to convert the PyTorch model, you'll need to use the conversion code in the [MCT tutorials folder](https://github.com/sony/model_optimization/tree/main/tutorials), so we'll clone the MCT repository to a local folder and only use that code. The installed MCT package will be used for quantization. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eda6ab0d8f0b6b56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone -b add_ported_effdet_keras_tutorial https://github.com/sony/model_optimization.git local_mct"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca9a743c0e7ba067"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"/content/local_mct\")\n",
    "from tutorials.resources.efficientdet import EfficientDetKeras, TorchWrapper\n",
    "from tutorials.resources.utils import load_state_dict"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38e460c939d89482"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Init dataset\n",
    "\n",
    "### Load COCO evaluation set"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f75abdac7950c038"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!wget -nc http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "!unzip -q -o annotations_trainval2017.zip -d /content/coco\n",
    "!echo Done loading annotations\n",
    "!wget -nc http://images.cocodataset.org/zips/val2017.zip\n",
    "!unzip -q -o val2017.zip -d /content/coco\n",
    "!echo Done loading val2017 images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bf50c7706331ba8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Init data loader and evaluation functions\n",
    "\n",
    "These functions were adapted from the [efficientdet-pytorch](https://github.com/rwightman/efficientdet-pytorch) repository."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6010ecf194d4a6a1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_coco_dataloader(batch_size=16, split='val', config=None):\n",
    "    \"\"\"\n",
    "    Get the torch data-loader and evaluation object\n",
    "    \"\"\"\n",
    "    root = '/content/coco'\n",
    "\n",
    "    args = dict(interpolation='bilinear', mean=None, std=None, fill_color=None)\n",
    "    dataset = create_dataset('coco', root, split)\n",
    "    input_config = resolve_input_config(args, config)\n",
    "    loader = create_loader(\n",
    "        dataset,\n",
    "        input_size=input_config['input_size'],\n",
    "        batch_size=batch_size,\n",
    "        use_prefetcher=True,\n",
    "        interpolation=input_config['interpolation'],\n",
    "        fill_color=input_config['fill_color'],\n",
    "        mean=input_config['mean'],\n",
    "        std=input_config['std'],\n",
    "        num_workers=0,\n",
    "        pin_mem=False,\n",
    "    )\n",
    "    evaluator = create_evaluator('coco', dataset, pred_yxyx=False)\n",
    "\n",
    "    return loader, evaluator\n",
    "\n",
    "\n",
    "def acc_eval(_model, batch_size=16, config=None):\n",
    "    # EValuate input model\n",
    "    val_loader, evaluator = get_coco_dataloader(batch_size=batch_size, config=config)\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    end = time()\n",
    "    last_idx = len(val_loader) - 1\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            output = _model(input, img_info=target)\n",
    "\n",
    "            evaluator.add_predictions(output, target)\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time() - end)\n",
    "            end = time()\n",
    "            if i % 10 == 0 or i == last_idx:\n",
    "                print(\n",
    "                    f'Test: [{i:>4d}/{len(val_loader)}]  '\n",
    "                    f'Time: {batch_time.val:.3f}s ({batch_time.avg:.3f}s, {input.size(0) / batch_time.avg:>7.2f}/s)  '\n",
    "                )\n",
    "\n",
    "    return evaluator.evaluate()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5833c805a1ca77aa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Keras model\n",
    "\n",
    "Create the Keras model and copy weights from pretrained PyTorch weights file. Saved as \"model.keras\"."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e589b01c6a45a9e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name = 'tf_efficientdet_lite0'\n",
    "config = get_efficientdet_config(model_name)\n",
    "\n",
    "model = EfficientDetKeras(config,\n",
    "                          pretrained_backbone=False\n",
    "                          ).get_model([*config.image_size] + [3])\n",
    "\n",
    "state_dict = torch.hub.load_state_dict_from_url(config.url, progress=False,\n",
    "                                                map_location='cpu')\n",
    "state_dict_numpy = {k: v.numpy() for k, v in state_dict.items()}\n",
    "load_state_dict(model, state_dict_numpy)\n",
    "\n",
    "model.save('/content/model.keras')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f1dacee7a949928"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate Keras model\n",
    "\n",
    "Wrap model in a Torch Module, so it can be evaluated with timm's evaluation code. We evaluate the model to verify the conversion succeeded and to compare it to the quantized model evaluation. The \"TorchWrapper\" object is a PyTorch module that serves as an API between the timm's Torch evaluation framework and the Keras model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef6b474a69358e03"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wrapped_model = TorchWrapper(model)\n",
    "\n",
    "float_map = acc_eval(wrapped_model, batch_size=64, config=config)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc2c87ab3460f395"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quantized Keras model\n",
    "\n",
    "The quantized model is saved as \"quant_model.keras\"."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aca80a0fc370eef3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loader, _ = get_coco_dataloader(split='val', config=config)\n",
    "\n",
    "\n",
    "def get_representative_dataset(n_iter):\n",
    "\n",
    "    def representative_dataset():\n",
    "        ds_iter = iter(loader)\n",
    "        for _ in range(n_iter):\n",
    "            t = next(ds_iter)[0]\n",
    "            yield [t.detach().cpu().numpy().transpose((0, 2, 3, 1))]\n",
    "\n",
    "    return representative_dataset\n",
    "\n",
    "\n",
    "quant_model, _ = mct.ptq.keras_post_training_quantization_experimental(model,\n",
    "                                                                       get_representative_dataset(20))\n",
    "quant_model.save('/content/quant_model.keras')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f1fa147c5a16df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate quantized Keras model\n",
    "\n",
    "Quantized Keras model evaluation applied the same as the original model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79ae299b0b019953"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wrapped_model = TorchWrapper(quant_model)\n",
    "\n",
    "quant_map = acc_eval(wrapped_model, batch_size=64, config=config)\n",
    "\n",
    "print(f' ===>> Float model mAP = {100*float_map:2.3f}, Quantized model mAP = {100*quant_map:2.3f}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f93b9b932fb39cc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
