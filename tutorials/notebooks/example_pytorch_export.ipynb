{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Export Quantized Keras Model\n",
        "\n",
        "[Run this tutorial in Google Colab](https://colab.research.google.com/github/reuvenperetz/model_optimization/blob/change-keras-serial-enum/tutorials/notebooks/example_pytorch_export.ipynb)\n",
        "\n",
        "\n",
        "To export a TensorFlow model as a quantized model, it is necessary to first apply quantization\n",
        "to the model using MCT:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UJDzewEYfSN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q git+https://github.com/reuvenperetz/model_optimization.git@change-keras-serial-enum"
      ],
      "metadata": {
        "id": "qNddNV6TEsX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.applications import ResNet50\n",
        "import model_compression_toolkit as mct\n",
        "\n",
        "# Create a model\n",
        "float_model = ResNet50()\n",
        "# Quantize the model. In order to export the model set new_experimental_exporter to True.\n",
        "# Notice that here the representative dataset is random for demonstration only.\n",
        "quantized_exportable_model, _ = mct.ptq.keras_post_training_quantization_experimental(float_model,\n",
        "                                                                                      representative_data_gen=lambda: [np.random.random((1, 224, 224, 3))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eheBYKxRDFgx",
        "outputId": "aa987180-861d-4eac-f91b-22cbb5f8866d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
            "102967424/102967424 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:Constrained Model Optimization:representative_data_gen generates a batch size of 1 which can be slow for optimization: consider increasing the batch size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "representative_data_gen generates a batch size of 1 which can be slow for optimization: consider increasing the batch size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running quantization parameters search. This process might take some time, depending on the model size and the selected quantization methods.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating quantization params:  69%|██████▉   | 86/125 [06:58<08:52, 13.66s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### keras\n",
        "\n",
        "The model will be exported as a tensorflow `.keras` model where weights and activations are quantized but represented using a float32 dtype.\n",
        "Two optional quantization formats are available: MCTQ and FAKELY_QUANT.\n",
        "\n",
        "#### MCTQ Quantization Format\n",
        "\n",
        "By default, `mct.exporter.keras_export_model` will export the quantized Keras model to\n",
        "a .keras model with custom quantizers from mct_quantizers module.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-n70LVe6DQPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile\n",
        "\n",
        "# Path of exported model\n",
        "_, keras_file_path = tempfile.mkstemp('.keras')\n",
        "\n",
        "# Export a keras model with mctq custom quantizers.\n",
        "mct.exporter.keras_export_model(model=quantized_exportable_model,\n",
        "                                save_model_path=keras_file_path)"
      ],
      "metadata": {
        "id": "PO-Hh0bzD1VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the model has the same size as the quantized exportable model as weights data types are float.\n",
        "\n",
        "#### Fakely-Quantized"
      ],
      "metadata": {
        "id": "Bwx5rxXDF_gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path of exported model\n",
        "_, keras_file_path = tempfile.mkstemp('.keras')\n",
        "\n",
        "# Use mode KerasExportSerializationFormat.KERAS for a .keras model\n",
        "# and QuantizationFormat.FAKELY_QUANT for fakely-quantized weights\n",
        "# and activations.\n",
        "mct.exporter.keras_export_model(model=quantized_exportable_model,\n",
        "                                save_model_path=keras_file_path,\n",
        "                                quantization_format=mct.exporter.QuantizationFormat.FAKELY_QUANT)"
      ],
      "metadata": {
        "id": "WLyHEEiwGByT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the fakely-quantized model has the same size as the quantized exportable model as weights data types are\n",
        "float.\n",
        "\n",
        "\n",
        "\n",
        "### TFLite\n",
        "The tflite serialization format export in two qauntization formats: INT8 and FAKELY_QUANT.\n",
        "\n",
        "#### INT8 TFLite\n",
        "\n",
        "The model will be exported as a tflite model where weights and activations are represented as 8bit integers."
      ],
      "metadata": {
        "id": "-L1aRxFGGFeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile\n",
        "\n",
        "# Path of exported model\n",
        "_, tflite_file_path = tempfile.mkstemp('.tflite')\n",
        "\n",
        "# Use mode KerasExportSerializationFormat.TFLITE for tflite model and quantization_format.INT8.\n",
        "mct.exporter.keras_export_model(model=quantized_exportable_model,\n",
        "                                save_model_path=tflite_file_path,\n",
        "                                serialization_format=mct.exporter.KerasExportSerializationFormat.TFLITE,\n",
        "                                quantization_format=mct.exporter.QuantizationFormat.INT8)"
      ],
      "metadata": {
        "id": "V4I-p1q5GLzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare size of float and quantized model:\n"
      ],
      "metadata": {
        "id": "SBqtJV9AGRzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Save float model to measure its size\n",
        "_, float_file_path = tempfile.mkstemp('.keras')\n",
        "float_model.save(float_file_path)\n",
        "\n",
        "print(\"Float model in Mb:\", os.path.getsize(float_file_path) / float(2 ** 20))\n",
        "print(\"Quantized model in Mb:\", os.path.getsize(tflite_file_path) / float(2 ** 20))\n",
        "print(f'Compression ratio: {os.path.getsize(float_file_path) / os.path.getsize(tflite_file_path)}')"
      ],
      "metadata": {
        "id": "LInM16OMGUtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Fakely-Quantized TFLite\n",
        "\n",
        "The model will be exported as a tflite model where weights and activations are quantized but represented as float.\n",
        "operators.\n",
        "\n",
        "##### Usage Example"
      ],
      "metadata": {
        "id": "9eVDoIHiGX5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path of exported model\n",
        "_, tflite_file_path = tempfile.mkstemp('.tflite')\n",
        "\n",
        "# Use mode KerasExportSerializationFormat.TFLITE for tflite model and QuantizationFormat.FAKELY_QUANT for fakely-quantized weights\n",
        "# and activations.\n",
        "mct.exporter.keras_export_model(model=quantized_exportable_model,\n",
        "                                save_model_path=tflite_file_path,\n",
        "                                serialization_format=mct.exporter.KerasExportSerializationFormat.TFLITE,\n",
        "                                quantization_format=mct.exporter.QuantizationFormat.FAKELY_QUANT)"
      ],
      "metadata": {
        "id": "0OYLAbI8Gawu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "Notice that the fakely-quantized model has the same size as the quantized exportable model as weights data types are\n",
        "float.\n"
      ],
      "metadata": {
        "id": "voOrtCroD-HE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb7e1572"
      },
      "source": [
        "Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n"
      ]
    }
  ]
}
