{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Export Quantized Pytorch Model\n",
        "\n",
        "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/pytorch/export/example_pytorch_export.ipynb)\n",
        "\n",
        "\n",
        "To export a Pytorch model as a quantized model, it is necessary to first apply quantization\n",
        "to the model using MCT:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UJDzewEYfSN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q mct-nightly"
      ],
      "metadata": {
        "id": "qNddNV6TEsX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to export your quantized model to onnx format, and use it for inference, some additional packages are needed. Notice, this is needed only for models exported to onnx, so this part can be skipped if this is not planned:"
      ],
      "metadata": {
        "id": "_w7xvHbcj1aV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q onnx onnxruntime onnxruntime-extensions"
      ],
      "metadata": {
        "id": "g10bFms8jzln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's start the export demonstration by quantizing the model using MCT:"
      ],
      "metadata": {
        "id": "Q36T6YpZkeTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import model_compression_toolkit as mct\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.models.mobilenetv2 import mobilenet_v2\n",
        "\n",
        "# Create a model\n",
        "float_model = mobilenet_v2()\n",
        "\n",
        "\n",
        "# Notice that here the representative dataset is random for demonstration only.\n",
        "def representative_data_gen():\n",
        "    yield [np.random.random((1, 3, 224, 224))]\n",
        "\n",
        "\n",
        "quantized_exportable_model, _ = mct.ptq.pytorch_post_training_quantization_experimental(float_model, representative_data_gen=representative_data_gen)"
      ],
      "metadata": {
        "id": "eheBYKxRDFgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### ONNX\n",
        "\n",
        "The model will be exported in ONNX format where weights and activations are represented as float. Notice onnx should be installed in order to export the model to an onnx model.\n",
        "\n",
        "There are two optional formats to choose: MCTQ or FAKELY_QUANT.\n",
        "\n",
        "#### MCTQ Quantization Format\n",
        "\n",
        "By default, `mct.exporter.pytorch_export_model` will export the quantized pytorch model to\n",
        "an onnx model with custom quantizers from mct_quantizers module.  \n",
        "\n"
      ],
      "metadata": {
        "id": "-n70LVe6DQPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path of exported model\n",
        "onnx_file_path = 'model_format_onnx_mctq.onnx'\n",
        "\n",
        "# Export onnx model with mctq quantizers.\n",
        "mct.exporter.pytorch_export_model(model=quantized_exportable_model,\n",
        "                                  save_model_path=onnx_file_path,\n",
        "                                  repr_dataset=representative_data_gen)"
      ],
      "metadata": {
        "id": "PO-Hh0bzD1VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the model has the same size as the quantized exportable model as weights data types are float.\n",
        "\n",
        "#### ONNX opset version\n",
        "\n",
        "By default, the used ONNX opset version is 15, but this can be changed using `onnx_opset_version`:"
      ],
      "metadata": {
        "id": "Bwx5rxXDF_gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Export onnx model with mctq quantizers.\n",
        "mct.exporter.pytorch_export_model(model=quantized_exportable_model,\n",
        "                                  save_model_path=onnx_file_path,\n",
        "                                  repr_dataset=representative_data_gen,\n",
        "                                  onnx_opset_version=16)"
      ],
      "metadata": {
        "id": "S9XtcX8s3dU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use exported model for inference\n",
        "\n",
        "To load and infer using the exported model, which was exported to an onnx file in MCTQ format, we will use `mct_quantizers` method `get_ort_session_options` during onnxruntime session creation."
      ],
      "metadata": {
        "id": "OygCt_iHQQiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mct_quantizers as mctq\n",
        "import onnxruntime as ort\n",
        "\n",
        "sess = ort.InferenceSession(onnx_file_path,\n",
        "                            mctq.get_ort_session_options(),\n",
        "                            providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
        "\n",
        "_input_data = next(representative_data_gen())[0].astype(np.float32)\n",
        "_model_output_name = sess.get_outputs()[0].name\n",
        "_model_input_name = sess.get_inputs()[0].name\n",
        "\n",
        "# Run inference\n",
        "predictions = sess.run([_model_output_name], {_model_input_name: _input_data})"
      ],
      "metadata": {
        "id": "VJL7KkLjRImb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fakely-Quantized\n",
        "\n",
        "To export a fakely-quantized model, use QuantizationFormat.FAKELY_QUANT:"
      ],
      "metadata": {
        "id": "Uf4SbpNC28GA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile\n",
        "\n",
        "# Path of exported model\n",
        "_, onnx_file_path = tempfile.mkstemp('.onnx')\n",
        "\n",
        "# Use QuantizationFormat.FAKELY_QUANT for fakely-quantized weights and activations.\n",
        "mct.exporter.pytorch_export_model(model=quantized_exportable_model,\n",
        "                                  save_model_path=onnx_file_path,\n",
        "                                  repr_dataset=representative_data_gen,\n",
        "                                  quantization_format=mct.exporter.QuantizationFormat.FAKELY_QUANT)"
      ],
      "metadata": {
        "id": "WLyHEEiwGByT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Notice that the fakely-quantized model has the same size as the quantized\n",
        "exportable model as weights data types are float.\n",
        "\n",
        "### TorchScript\n",
        "\n",
        "The model will be exported in TorchScript format where weights and activations are\n",
        "quantized but represented as float (fakely quant)."
      ],
      "metadata": {
        "id": "-L1aRxFGGFeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path of exported model\n",
        "_, torchscript_file_path = tempfile.mkstemp('.pt')\n",
        "\n",
        "\n",
        "# Use mode PytorchExportSerializationFormat.TORCHSCRIPT a torchscript model\n",
        "# and QuantizationFormat.FAKELY_QUANT for fakely-quantized weights and activations.\n",
        "mct.exporter.pytorch_export_model(model=quantized_exportable_model,\n",
        "                                  save_model_path=torchscript_file_path,\n",
        "                                  repr_dataset=representative_data_gen,\n",
        "                                  serialization_format=mct.exporter.PytorchExportSerializationFormat.TORCHSCRIPT,\n",
        "                                  quantization_format=mct.exporter.QuantizationFormat.FAKELY_QUANT)"
      ],
      "metadata": {
        "id": "V4I-p1q5GLzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the fakely-quantized model has the same size as the quantized exportable model as weights data types are\n",
        "float."
      ],
      "metadata": {
        "id": "SBqtJV9AGRzN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb7e1572"
      },
      "source": [
        "Copyright 2024 Sony Semiconductor Israel, Inc. All rights reserved.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n"
      ]
    }
  ]
}
