:orphan:

.. _ug-hardware_representation:


=================================
hardware_representation Module
=================================

MCT can be configured to quantize and optimize models for different hardware settings.
For example, when using qnnpack backend for Pytorch model inference, Pytorch quantization configuration
uses per-tensor weights quantization for Conv2d, while when using tflite modeling, Tensorflow uses
per-channel weights quantization for Conv2D.
This can be addressed in MCT by using the hardware_representation module, that can define many different
parameters that are hardware-related.

OpQuantizationConfig
======================
.. autoclass:: model_compression_toolkit.hardware_representation.OpQuantizationConfig




